{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import zipfile, tarfile, os\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zwraca pogrupowane stacje po id w kolumnie kodSH w dataframie 'data'\n",
    "def groupbyKodSH(station, data):\n",
    "    try:\n",
    "        res = data.groupby(data.KodSH).get_group(station)\n",
    "        return res\n",
    "    except KeyError:\n",
    "        print('stacja ' + str(station) + ' nie posiada takich danych')\n",
    "        pass\n",
    "\n",
    "# importuje pliki z url, tworzy nowy folder 'data' lub czysci istniejacy i rozpakowuje archiwum\n",
    "def importData(url):\n",
    "    path = \"D:\\\\STUDIA_5_SEM\\\\pag_2\\\\cw2\\\\csv\"\n",
    "    filename = url.split('/')[-1]\n",
    "\n",
    "    if filename[-3:] == \"zip\":\n",
    "        isZip = True\n",
    "    elif filename[-3:] == \"tar\":\n",
    "        isZip = False\n",
    "    else:\n",
    "        print(\"Błąd, niewłaściwy format pliku\")\n",
    "        return\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    elif len(os.listdir(path)) != 0:\n",
    "        dictContent = os.listdir(path)\n",
    "        for x in dictContent:\n",
    "            file = os.path.join(path, x)\n",
    "            os.unlink(file)\n",
    "\n",
    "    urllib.request.urlretrieve(url, os.path.join(path, filename))\n",
    "\n",
    "    if isZip:\n",
    "        zipData = zipfile.ZipFile(os.path.join(path, filename))\n",
    "        zipData.extractall(path)\n",
    "        zipData.close()\n",
    "    else:\n",
    "        tarData = tarfile.TarFile(os.path.join(path, filename))\n",
    "        tarData.extractall(path)\n",
    "        tarData.close()\n",
    "        \n",
    "    os.remove(os.path.join(path, filename))\n",
    "\n",
    "# wczytuje wszystkie pliki csv w folderze, zwraca liste dataframe'ow(csv) i stacji\n",
    "def readCsv(path, layer):\n",
    "    csvList = listdir(path)\n",
    "    dfList = []\n",
    "    stations_list = []\n",
    "    for x in csvList:\n",
    "        if x == layer:\n",
    "            csvData = pd.read_csv(path + '\\\\' + x, delimiter=';', header=None, low_memory=False, decimal=',')\n",
    "            df = pd.DataFrame(csvData)\n",
    "            df.pop(4)\n",
    "            columns = [\"KodSH\", \"ParametrSH\", \"Data\", \"Wartosc\"]\n",
    "            df.columns = columns\n",
    "            dfList.append(df)\n",
    "            stations = df[\"KodSH\"].drop_duplicates().to_list()\n",
    "            stations_list.append(stations)\n",
    "    return dfList[0], stations_list\n",
    "\n",
    "def getTimeOfSunriseAndSunset(db, long, lat, random_date):\n",
    "    try:\n",
    "        station = LocationInfo(lat, long)\n",
    "        res = pd.DataFrame()\n",
    "\n",
    "        for i in range (0, db.loc[db['Data'] < str(random_date.strftime('%Y-%m-%d %H:%M')), 'Wartosc'].count()):\n",
    "\n",
    "            curr_data = DF_date_to_datetime(db.iloc[i, 2])\n",
    "            val = db.iloc[i, 3]\n",
    "            s = sun(station.observer, date = curr_data)\n",
    "\n",
    "            if curr_data < s['sunrise'].replace(tzinfo = None):\n",
    "                new_dt = curr_data + datetime.timedelta(days = -1)\n",
    "                res = pd.concat([res, pd.DataFrame({'Data' : [new_dt.strftime('%Y-%m-%d')], 'time' : ['noc'], 'srednia' : [val]})], ignore_index = True)\n",
    "\n",
    "            elif curr_data >= s['sunrise'].replace(tzinfo = None) and curr_data < s['sunset'].replace(tzinfo = None):\n",
    "                res = pd.concat([res, pd.DataFrame({'Data' : [curr_data.strftime('%Y-%m-%d')], 'time' : ['dzien'], 'srednia' : [val]})], ignore_index = True)\n",
    "\n",
    "            elif curr_data >= s['sunset'].replace(tzinfo = None):\n",
    "                res = pd.concat([res, pd.DataFrame({'Data' : [curr_data.strftime('%Y-%m-%d')], 'time' : ['noc'], 'srednia' : [val]})], ignore_index = True)\n",
    "\n",
    "        mediana = res.groupby([res.Data, res.time]).median().srednia.tolist()\n",
    "        final = res.groupby([res.Data, res.time]).mean()\n",
    "        final['mediana'] = mediana\n",
    "\n",
    "        return final\n",
    "        \n",
    "    except AttributeError:\n",
    "        print('brak stacji')\n",
    "        pass\n",
    "\n",
    "### rozkładanie daty z DF na czynniki pierwsze\n",
    "def DF_date_to_datetime(date): # date -> pełna data z DF danej stacji pomiarowej np. date = csv_1_stacja_1.iloc[0,2]\n",
    "    dash_ind = [x for x in range(len(date)) if date.find('-', x) == x]\n",
    "    space_ind = date.find(\" \")\n",
    "    min_ind = date.find(\":\")\n",
    "    Y = int(date.strip()[0 : dash_ind[0]])\n",
    "    M = int(date.strip()[dash_ind[0]+1 : dash_ind[1]])\n",
    "    D = int(date.strip()[dash_ind[1]+1 : space_ind])\n",
    "    H = int(date.strip()[space_ind+1 : min_ind])\n",
    "    Min = int(date.strip()[min_ind+1 : len(date)])\n",
    "    return(datetime.datetime(Y, M, D, H, Min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zwraca dataframe stacji wczytanych z geojsona\n",
    "def get_station_loc(pathnameToGeoJson):\n",
    "    effacility = gpd.read_file(pathnameToGeoJson)\n",
    "    station_loc = gpd.GeoDataFrame(effacility.pop('ifcid'))\n",
    "    loc = effacility.pop('geometry')\n",
    "    station_loc['geometry'] = loc\n",
    "    return station_loc\n",
    "\n",
    "# plik .shp jednostki administracyjnej, wyodrębnienie nazwy i geometrii, zwraca dataframe z danymi jednostek adm.\n",
    "def get_adm_geom(pathnameToShp):\n",
    "    adm = gpd.read_file(pathnameToShp)\n",
    "    adm_data = gpd.GeoDataFrame(adm.pop('name'))\n",
    "    geom = adm.pop('geometry')\n",
    "    adm_data['geometry'] = geom\n",
    "    return adm_data\n",
    "\n",
    "# dopasowanie do stacji z pliku .geojson województwa(i powiaty jesli chcemy) w którym się znajdują\n",
    "def match_station_adm(station_loc, woj_data, pow_data=None):\n",
    "    in_woj = []\n",
    "    in_pow = []\n",
    "    ind = 1\n",
    "    only_woj = True\n",
    "    if pow_data is not None:\n",
    "        only_woj = False\n",
    "\n",
    "    for el in station_loc.index:\n",
    "        w = str(woj_data.name.loc[woj_data.geometry.contains(station_loc.geometry[el])])\n",
    "        w1 = w.lstrip('0123456789')\n",
    "        w2 = w1.rstrip('\\nName: name,dtype: object')\n",
    "        ### taki .rstrip() ucina 'e' z nazwy każdego województwa, więc trzeba je spowrotem dodać\n",
    "        in_woj.append(w2 + 'e')\n",
    "\n",
    "        if not only_woj:\n",
    "            p = str(pow_data.name.loc[pow_data.geometry.contains(station_loc.geometry[el])])\n",
    "            p1 = p.lstrip('0123456789')\n",
    "            p2 = p1.rstrip('\\nName: name,dtype: object')\n",
    "            in_pow.append(p2)\n",
    "\n",
    "        ### jeśli stacja leży poza granicami Polski\n",
    "        if len(in_woj) != ind:\n",
    "            in_woj.append(\"-\")\n",
    "            in_pow.append('-')\n",
    "        ind+=1\n",
    "\n",
    "    station_loc['woj'] = in_woj\n",
    "    if not only_woj:\n",
    "        station_loc['pow'] = in_pow\n",
    "    return station_loc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "konfiguracja MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json\n",
    "import geojson\n",
    "from shapely.geometry.point import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://pag2:giPAG2@cluster0.jrtrict.mongodb.net/?retryWrites=true&w=majority\")\n",
    "db = client.pag2\n",
    "\n",
    "imgw = db.imgw\n",
    "effacility = db.effacility\n",
    "stations = db.stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data_one(collection, feature, match_element, project_features):\n",
    "    el = list(collection.aggregate([\n",
    "    {\n",
    "        '$unwind' : {\n",
    "            'path' : '$features',\n",
    "            'includeArrayIndex': 'features.id'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$match' : {\n",
    "            feature : match_element\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project' : {\n",
    "            project_features[0] : 1,\n",
    "            project_features[1] : 1\n",
    "        }\n",
    "    }\n",
    "    ]))\n",
    "    res = el[0]\n",
    "    res.pop('_id')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data_many(collection, match_element, project_features):\n",
    "    el = list(collection.aggregate([\n",
    "    {\n",
    "        '$unwind' : {\n",
    "            'path' : '$features',\n",
    "            'includeArrayIndex': 'features.id'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project' : {\n",
    "            project_features[0] : 1,\n",
    "            project_features[1] : 1\n",
    "        }\n",
    "    }\n",
    "    ]))\n",
    "    res = []\n",
    "    for item in el:\n",
    "        item.pop('_id')\n",
    "\n",
    "    ### WYSZUKIWANIE GEOMETRII ITERUJĄC PO ZWYKŁEJ LIŚCIE\n",
    "    ### MOŻNA DO BAZY DANYCH WRZUCIĆ WSZYSTKIE STACJE I POTEM UŻYĆ '$GeoWithin' DO ICH WYBIERANIA ALE OPERACJA DODAWANIA I USUWANIA STACJI W MONGODB TRWA PO ~3 MINUTY\n",
    "        if Point(item.get('features', {}).get('geometry', {}).get('coordinates')).within(Polygon(match_element)):\n",
    "            res.append(item)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STACJE POMIAROWE W POLSCE\n",
    "stat = gpd.read_file('D:\\\\STUDIA_5_SEM\\\\pag_2\\\\cw2\\\\Dane\\\\effacility.geojson')\n",
    "stat = json.loads(stat.to_json())\n",
    "effacility.insert_one(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usunięcie stacji z MongoDB\n",
    "effacility.delete_one(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WOJEWÓDZTWA\n",
    "woj = gpd.read_file('D:\\\\STUDIA_5_SEM\\\\pag_2\\\\cw2\\\\Dane\\\\woj.shp')\n",
    "wojewodztwa = json.loads(woj.to_json())\n",
    "imgw.insert_one(wojewodztwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usunięcie województw z MongoDB\n",
    "imgw.delete_one(wojewodztwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usunięcie wyników obliczeń z MongoDB -> datę trzeba wpisać odręcznie\n",
    "stations.delete_many({\"srednia.('2022-09-01', 'dzien')\" : {'$gt': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamknięcie połączenia z bazą danych\n",
    "client.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "konfiguracja Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "r = redis.ConnectionPool(\n",
    "  host = '127.0.0.1',\n",
    "  port = 6379,\n",
    "  db = 0\n",
    ")\n",
    "\n",
    "db_redis = redis.Redis(connection_pool=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputIntoRedis(df, idstacji, date):\n",
    "    data = df.to_dict('list')\n",
    "    n = 0\n",
    "    i = 0\n",
    "    indexStacji = []\n",
    "    indexStacji.append(n)\n",
    "    while i+1 < len(idstacji):\n",
    "        if idstacji[i] != data['KodSH'][n]:\n",
    "            indexStacji.append(n)\n",
    "            i += 1\n",
    "        n += 1\n",
    "    for a in range(len(indexStacji)):\n",
    "        if a == len(indexStacji)-1:\n",
    "            pomiar = [str(x) for x in data['Wartosc'][indexStacji[a]:]]\n",
    "            czas = data['Data'][indexStacji[a]:]\n",
    "            czas = ','.join(czas)\n",
    "            pomiar = ','.join(pomiar)\n",
    "            klucz = str(idstacji[a]) + '#' + data['ParametrSH'][0] + '#' + date\n",
    "            db_redis.hmset(klucz, {'data': czas, 'wartosc': pomiar})\n",
    "        else:\n",
    "            pomiar = [str(x) for x in data['Wartosc'][indexStacji[a]:indexStacji[a+1]]]\n",
    "            czas = data['Data'][indexStacji[a]:indexStacji[a+1]]\n",
    "            czas = ','.join(czas)\n",
    "            pomiar = ','.join(pomiar)\n",
    "            klucz = str(idstacji[a]) + '#' + data['ParametrSH'][0] + '#' + date\n",
    "            db_redis.hmset(klucz, {'data': czas, 'wartosc': pomiar})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArythmeticMeanMaxMin(key, lat, long, redisDB):\n",
    "    try:\n",
    "        val, date = redisDB.hmget(key, ['wartosc', 'data'])\n",
    "        date = date.decode('utf-8').split(',')\n",
    "        val = val.decode('utf-8').split(',')\n",
    "        val = [float(x) for x in val]\n",
    "        id = key.split('#')[0]\n",
    "        station = LocationInfo(lat,long)\n",
    "        res = pd.DataFrame()\n",
    "        for i in range(len(date)):\n",
    "            currentDate = DF_date_to_datetime(date[i])\n",
    "            s = sun(station.observer, date=currentDate)\n",
    "            if currentDate < s['sunrise'].replace(tzinfo=None):\n",
    "                newDate = currentDate + datetime.timedelta(days=-1)\n",
    "                res = pd.concat(\n",
    "                    [res, pd.DataFrame({'IdStacji': id, 'Data': [newDate.strftime('%Y-%m-%d')], 'pora_dnia': ['noc'], 'srednia': [val[i]]})],\n",
    "                    ignore_index=True)\n",
    "\n",
    "            elif currentDate >= s['sunrise'].replace(tzinfo=None) and currentDate < s['sunset'].replace(tzinfo=None):\n",
    "                res = pd.concat(\n",
    "                    [res, pd.DataFrame({'IdStacji': id, 'Data': [currentDate.strftime('%Y-%m-%d')], 'pora_dnia': ['dzień'], 'srednia': [val[i]]})],\n",
    "                    ignore_index=True)\n",
    "\n",
    "            elif currentDate >= s['sunset'].replace(tzinfo=None):\n",
    "                res = pd.concat(\n",
    "                    [res, pd.DataFrame({'IdStacji': id, 'Data': [currentDate.strftime('%Y-%m-%d')], 'pora_dnia': ['noc'], 'srednia': [val[i]]})],\n",
    "                    ignore_index=True)\n",
    "\n",
    "        mediana = res.groupby([res.IdStacji, res.Data, res.pora_dnia]).median(numeric_only = True).srednia.tolist()\n",
    "        final = res.groupby([res.IdStacji, res.Data, res.pora_dnia]).mean(numeric_only = True)\n",
    "        final['mediana'] = mediana\n",
    "\n",
    "        return final\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1719be7d630e7e0894a4346e00239747771550a505546fd5ba4d63ceaa06c07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
